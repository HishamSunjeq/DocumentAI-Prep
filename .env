DATA_FOLDER_PATH=./data
OUTPUT_FOLDER_PATH=./output

OCR_OUTPUT_FOLDER_PATH=./output/ocr_output

CHUNKED_INPUT_FOLDER_PATH=./output/ocr_output
CHUNKED_OUTPUT_FOLDER_PATH=./output/chunked_output

# Embedding Configuration
# Options: "ollama" or "sentence_transformer"
EMBEDDING_TYPE=sentence_transformer

# Ollama Configuration
OLLAMA_MODEL_NAME=nomic-embed-text:latest
OLLAMA_URL=http://localhost:11434

# SentenceTransformer Configuration
# Options: "all-MiniLM-L6-v2", "all-MiniLM-L12-v2", "multi-qa-MiniLM-L6-cos-v1", etc.
# We are using "multi-qa-MiniLM-L6-cos-v1" for better performance becuse it is optimized for QA.
SENTENCE_TRANSFORMER_MODEL=multi-qa-MiniLM-L6-cos-v1
MODELS_FOLDER_PATH=./models

# Legacy support (will use OLLAMA_MODEL_NAME if EMBEDDING_TYPE=ollama)
CHUNKED_MODEL_NAME=nomic-embed-text:latest

# Q&A Generation Configuration
QA_INPUT_FOLDER=./output/chunked_output
QA_OUTPUT_FOLDER=./output/qa_pairs
QA_OLLAMA_HOST=http://localhost:11434
QA_OLLAMA_MODEL=mistral
QA_PAIRS_PER_CHUNK=3
QA_FORCE_GPU=true

GPU_DEVICE_ID=0
CUDA_VISIBLE_DEVICES=0

# GPU Optimization Settings for Ollama
OLLAMA_HOST=127.0.0.1:11434
OLLAMA_NUM_PARALLEL=1
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_GPU_OVERHEAD=0
CUDA_VISIBLE_DEVICES=0

# QA Generation GPU Settings
QA_OLLAMA_HOST=http://127.0.0.1:11434
QA_OPTIMIZE_OLLAMA=true
QA_BATCH_SIZE=3
QA_ENABLE_MONITORING=true
